{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f46f12aa-e3f8-4eca-9628-68ce7a0d7a2c",
   "metadata": {},
   "source": [
    "# 02 - Latent Semantic Analysis (LSA)\n",
    "This notebook performs Latent Semantic Analysis on the governance data set. It assumes the data was cleaned and placed in the cache by the \"_00 - Preprocess the Governance Data Set__\" notebook.\n",
    "\n",
    "The code in this notebook is largely based on the [Introduction to Latent Semantic Analysis](https://www.youtube.com/playlist?list=PLroeQp1c-t3qwyrsq66tBxfR6iX6kSslt) youtube series by Joshua Cook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab985928-7bf2-4138-bb92-be26c7422969",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Dependencies and Imports\n",
    "Here we import the libraries we need to perform our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa0abc4-a449-4fa7-a3c3-762de699e709",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn pandas seaborn graphviz wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cfb4b9-55cc-488d-8c15-d50291a53785",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "print(\"python=={}\".format(re.sub(r'\\s.*', '', sys.version)))\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import __version__ as sklearn__version__\n",
    "print(f\"scikit-learn=={sklearn__version__}\")\n",
    "\n",
    "import pandas as pd\n",
    "print(f\"pandas=={pd.__version__}\")\n",
    "ROW    = 0\n",
    "COLUMN = 1\n",
    "STRING = 'string'\n",
    "OBJECT = 'object'\n",
    "NUMBER = 'number'\n",
    "CATEGORY = 'category'\n",
    "INTEGER = 'integer'\n",
    "UNSIGNED = 'unsigned'\n",
    "FLOAT = 'float'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import __version__ as matplotlib__version__\n",
    "print(f\"matplotlib=={matplotlib__version__}\")\n",
    "\n",
    "import seaborn as sns\n",
    "CMAP_VLAG = sns.color_palette(\"vlag\", as_cmap=True)\n",
    "print(f\"seaborn=={sns.__version__}\")\n",
    "\n",
    "from graphviz import __version__ as graphviz__version__\n",
    "print(f\"graphviz=={graphviz__version__}\")\n",
    "from graphviz import Source as dot_graph\n",
    "\n",
    "import numpy as np\n",
    "print(f\"numpy=={np.__version__}\")\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import __version__ as wordcloud__version__\n",
    "print(f\"wordcloud=={wordcloud__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467f68d7-8077-42c0-a2ce-0806e5f2625c",
   "metadata": {},
   "source": [
    "---\n",
    "## Process Steps\n",
    "The image below shows the process steps that we are taking in this notebook. The values for `min_df` and `max_df` are explained elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227efff5-c259-4fef-84b9-06b7616f5017",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_graph('''\n",
    "digraph {\n",
    "  rankdir=LR;\n",
    "  node [fontname=\"georgia; serif\" fontsize=9, margin=0 arrowhead=normal arrowtail=dot];\n",
    "  edge [arrowsize=0.75]\n",
    "\n",
    "  node [shape=folder]; documents;\n",
    "  node [shape=oval]; vec [label=\"count vectorizer\", color=grey fontcolor=grey style=dotted];\n",
    "      tfidf [label=\"TF-IDF vectorizer\"]; svd [label=\"singular value decomposition\"];\n",
    "  node [shape=plain]; min_df [label=\"min_df=15\"]; max_df [label=\"min_df=85%\"];\n",
    "      sublinear_tf [label=\"sublinear_tf=True\"]; n_components [label=\"n_components=2\"]; dot0 [label=\" \"];\n",
    "  node [shape=box]; dtm [label=\"document term matrix\"]; dict [label=\"dictionary\"];\n",
    "      topic_encodings [label=\"topic encodings\"]; encoding_matrix [label=\"encoding matrix\"]; explained_variance_sum [label=\"sum(explained variance)\"];\n",
    "\n",
    "  documents -> corpus;\n",
    "  corpus -> vec [color=grey style=dotted arrowhead=vee arrowsize=0.5];\n",
    "  corpus -> tfidf;\n",
    "  sublinear_tf -> tfidf;\n",
    "  min_df -> tfidf;\n",
    "  max_df -> tfidf;\n",
    "  vec -> dtm [color=grey style=dotted arrowhead=vee arrowsize=0.5];\n",
    "  tfidf -> dtm;\n",
    "  vec -> dict [color=grey style=dotted arrowhead=vee arrowsize=0.5];\n",
    "  tfidf -> dict;\n",
    "  dtm -> svd;\n",
    "  dict -> svd;\n",
    "  n_components -> svd;\n",
    "  svd -> topic_encodings;\n",
    "  svd -> encoding_matrix;\n",
    "  svd -> explained_variance_sum;\n",
    "}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aeef7a-88e0-4953-aeb7-08f1c8211b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the caller is expected to select the right column from their data frame and\n",
    "# pass it in as a series.\n",
    "def compose_document_term_matrix(series, min_df, max_df, sublinear_tf):\n",
    "    vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, sublinear_tf=sublinear_tf)\n",
    "    dt_matrix  = vectorizer.fit_transform(series)\n",
    "    dictionary = vectorizer.get_feature_names_out()\n",
    "\n",
    "    return pd.DataFrame(index=series.index, columns=dictionary, data=dt_matrix.toarray()), \\\n",
    "           dictionary\n",
    "\n",
    "def perform_lsa(dt_matrix, dictionary, n_components, column_names=None):\n",
    "    svd = TruncatedSVD(n_components=n_components)\n",
    "    explained_variance_sum = svd.fit(dt_matrix).explained_variance_.sum()\n",
    "    lsa = svd.transform(dt_matrix)\n",
    "    if not column_names:\n",
    "        column_names = [f\"topic {n}\" for n in range(lsa.shape[1])]\n",
    "\n",
    "    return pd.DataFrame(index=dt_matrix.index, data=lsa, columns=column_names), \\\n",
    "           pd.DataFrame(index=column_names, data=svd.components_, columns=dictionary).T, \\\n",
    "           explained_variance_sum           \n",
    "\n",
    "def n_most_significant(topic, n_terms, encoding_matrix):\n",
    "    df = encoding_matrix[topic]\n",
    "    return df.iloc[df.abs().argsort()][::-1][:n_terms].to_frame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40593b20-f2a1-4cd3-9960-81db5152ba01",
   "metadata": {},
   "source": [
    "---\n",
    "## Apply LSA to the Governance Data Set\n",
    "We apply LSA to the subset _duurzaamheidsvisie_ (DV) documents from the governance data set.\n",
    "\n",
    "### Load the DV Documents from the Governance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095324b3-41b1-4542-92f1-03b93d361469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CACHE_DIR = '../cache/Governance'\n",
    "\n",
    "GLOB_DV = CACHE_DIR + '/GM????DV??.txt'\n",
    "\n",
    "# take a glob and make it iterable. We cannot use globs as objects, since these get\n",
    "# \"exhausted\" when you iterate over them.\n",
    "# https://stackoverflow.com/questions/51108256/how-to-take-a-pathname-string-with-wildcards-and-resolve-the-glob-with-pathlib\n",
    "def expand_glob(glob):\n",
    "    p = Path(glob)\n",
    "    return Path(p.parent).expanduser().glob(p.name)\n",
    "\n",
    "print(f\"'Duurzaamheidsvisie' documents = {GLOB_DV}\")\n",
    "\n",
    "DOCUMENT_BODY = 'body'\n",
    "\n",
    "def load_corpus_as_dataframe(glob):\n",
    "    df = pd.DataFrame(data   =[file.read_text() for file in expand_glob(glob)],\n",
    "                      index  =[file.stem        for file in expand_glob(glob)],\n",
    "                      columns=[DOCUMENT_BODY])\n",
    "    df[DOCUMENT_BODY] = df[DOCUMENT_BODY].astype(STRING)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098fba12-fd4d-43e2-be25-41f8cbfc76ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_DF = 15   # count\n",
    "MAX_DF = 0.85 # percent\n",
    "SUBLINEAR_TF = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279e5f8a-f7a4-4745-8d16-623aec4d3b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = load_corpus_as_dataframe(GLOB_DV)\n",
    "corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219fded1-fbf7-4f27-9392-f70a0c092c89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5ebb5f-9b7f-4864-896b-fd660a3a2aaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "document_term_matrix, dictionary = compose_document_term_matrix(corpus[DOCUMENT_BODY], min_df=MIN_DF, max_df=MAX_DF, sublinear_tf=SUBLINEAR_TF)\n",
    "document_term_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc14a6a-53f3-406b-a8f2-af88303bd97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc232f7-387b-4b30-a9a1-f8e93ae1bbaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_encodings, encoding_matrix, _ = perform_lsa(document_term_matrix, dictionary, 2)\n",
    "topic_encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5288569c-3c98-4184-94db-e34b843f24cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30744ad-c812-4403-bc4a-88b72f4df6ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_most_significant(\"topic 0\", 20, encoding_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bca3db-8b4a-4a33-a865-03aceb2b13b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_most_significant(\"topic 1\", 20, encoding_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cae08e9-d7aa-431e-b861-921945db49a3",
   "metadata": {},
   "source": [
    "### Determine the correct number of topics using latent sementic analysis\n",
    "Explained variance based on https://stackoverflow.com/questions/69091520/determine-the-correct-number-of-topics-using-latent-semantic-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f63808-d545-4888-bbae-15b7458a49c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = []\n",
    "test = range(1, 255)\n",
    "\n",
    "for n in test: #removed the loop for reasons of performance\n",
    "    _, _, explained_variance_sum = perform_lsa(document_term_matrix, dictionary, n)\n",
    "    performance.append(explained_variance_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23899322",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "plt.plot(test, performance, 'ro--')\n",
    "plt.title('cumulative explained variance ratio by n-components')\n",
    "plt.xlim(1, 255)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f71d221",
   "metadata": {},
   "source": [
    "70 to 80% of the explained variance ratio is reached with 125 to 150 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5248c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "plt.plot(test, np.gradient(performance), 'ro--')\n",
    "plt.title('explained variance ratio by n-components')\n",
    "plt.xlim(0, 25) # zoom to 25 to see the changes in the first components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836c3468",
   "metadata": {},
   "source": [
    "The graph shows that the explained variance of the LSA model is about 75% at maximum of 253 topics (= number of documents). This low explained variance is probably due to the relative few but lengthy documents.\n",
    "The explained variance flattens around 4 topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a979fb1",
   "metadata": {},
   "source": [
    "### Determining the beste parameters for the TF-IDF matrix in the LSA context\n",
    "Below a matrix is constructed for different values of the min_df (absolute values) and max_df (relative values) settings for constructing the TF-IDF matrix. These are evaluated for the total explained variance and the explained variance in the first 5 topics from the LSA. Also the topic composition of the resulting topics are compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b26a2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_ls = [0, 5, 10, 15, 20, 25, 35, 50]\n",
    "max_ls = [0.5, 0.75, 0.85, 0.9, 0.95, 1.0]\n",
    "dimensions = pd.DataFrame(index=min_ls, columns=max_ls)\n",
    "exvar_tot =  pd.DataFrame(index=min_ls, columns=max_ls)\n",
    "exvar_4 =    pd.DataFrame(index=min_ls, columns=max_ls)\n",
    "words = [[None] * len(max_ls)] * len(min_ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b1a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for min_df in min_ls:\n",
    "    c = 0\n",
    "    for max_df in max_ls:\n",
    "        dtm, dict = compose_document_term_matrix(corpus[DOCUMENT_BODY], min_df=min_df, max_df=max_df, sublinear_tf=SUBLINEAR_TF)\n",
    "        dimensions.loc[min_df, max_df] = dict.size\n",
    "\n",
    "        # calculate total variance, using n=255\n",
    "        _, _, expl_variance_sum = perform_lsa(dtm, dict, 255)\n",
    "        exvar_tot.loc[min_df, max_df] = expl_variance_sum\n",
    "\n",
    "        # calculate variance at 5 topics, using n=4\n",
    "        _, enc_matrix, expl_variance_sum = perform_lsa(dtm, dict, 4)\n",
    "        words[i][c] = enc_matrix\n",
    "        exvar_4.loc[min_df, max_df] = expl_variance_sum\n",
    "\n",
    "        c += 1\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c43bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d533e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exvar_tot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e27cc7c",
   "metadata": {},
   "source": [
    "High total explained variances are achieved for low values of both `min_df` and `max_df`. For `min_df` this is the result of including words that are unique for only a few documents. For `max_df` it is most likely because the total number of word occurances are reduces, which reduces the total variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee91773",
   "metadata": {},
   "outputs": [],
   "source": [
    "exvar_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b7c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "exvar_4 / exvar_tot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a958fae7",
   "metadata": {},
   "source": [
    "The first five topics explain most of the variance with a high max_df and low min_df. The explained variance ratio is largest at high max_df and high max_df, however this ratio reduces only slightly for medium values of max_df. So the influence of max_df is mainly influenced by the higher total explained variance. The effect of min_df is explained by less unique words for the documents (so the other topics are less able to explain variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fa68de",
   "metadata": {},
   "source": [
    "### Influence of `min_df` and `max_df` on Topic Interpretation\n",
    "Below the top 20 words in each topic are listed in order to be able to interprete the common theme within the topic. The influence of different min_df and max_df settings can be investegated by changing the index of the words matrix (see matrices above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04951c56",
   "metadata": {},
   "source": [
    "Topic labelling seems quite stable for most options of `min_df` and `max_df`. Only values of `max_df<=0.75` the label of the two last topics changes. These two topics seems to have quite a similar theme. It is concluded that low values of max_df results in less intepretable topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8dfbc",
   "metadata": {},
   "source": [
    "Topic composition is influenced only slightly by different values of `max_df` and `min_df`. Low values of `max_df` seems to reduce the explainability of the topics.\n",
    "\n",
    "From this analysis it can be concluded that medium values for both max_df and min_df results in a good balance between total explained variance and the explained variance of the first five topics. For our analysis it is proposed to use `min_df=15` and `max_df=0.85`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41883264",
   "metadata": {},
   "source": [
    "### Topic assessment with 4 topics\n",
    "According to the elbow method 4 topics are optimal to assess the information in the documents. Below these 4 topics are assessed for their content and value for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e370bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_encodings, encoded_matrix, _ = perform_lsa(document_term_matrix, dictionary, 4,\n",
    "                                                ['samenleving', 'milieu', 'mens en natuur', 'energie'])\n",
    "topic_encodings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c7303",
   "metadata": {},
   "source": [
    "The first 5 topics only account for 6.6% of the explained variance between the documents.\n",
    "\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a3e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c3553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(topic_encodings);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5ee8b1-ae7e-445c-a69e-94da75b5b68e",
   "metadata": {},
   "source": [
    "---\n",
    "## Generate Word Clouds per Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f978a23e-2e5f-45c7-9064-570ce9bd823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_of_topic(series):\n",
    "    print(series.sort_values(ascending=False)[:20])\n",
    "    cloud = WordCloud(background_color=\"white\", max_words=50).generate_from_frequencies(series.sort_values(ascending=False))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(cloud);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cd8db8-6cae-4000-a972-f3cb6c321819",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_of_topic(encoded_matrix['samenleving'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff42321-1b5c-4436-a872-9b0822d03d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_of_topic(encoded_matrix['milieu'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d3aa8-ea37-4834-b501-cb3f926e6c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_of_topic(encoded_matrix['mens en natuur'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c545ae67-d1e9-47bb-8d3f-45e9f3eff3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_of_topic(encoded_matrix['energie'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0195fdf7-841f-451a-91c4-96eef5178e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
